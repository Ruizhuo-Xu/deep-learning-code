{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMax:\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if x.ndim == 1:\n",
    "            x = x.unsqueeze(dim=0)\n",
    "        max_logits = torch.max(x, dim=-1, keepdim=True).values  # (B, N) -> (B, 1)\n",
    "        exp_logits = torch.exp(x - max_logits)  # (B, N)\n",
    "        return exp_logits / torch.sum(exp_logits, dim=-1, keepdim=True)\n",
    "\n",
    "    def backward(self, y: torch.Tensor, grad_output: torch.Tensor = None):\n",
    "        # y is the output from the forward pass (softmax output)\n",
    "        # grad_output is the gradient of the loss with respect to the output of softmax\n",
    "        \n",
    "        # Compute the gradient of the softmax function\n",
    "        batch_size, num_classes = y.shape\n",
    "        \n",
    "        if grad_output is not None:\n",
    "            # Reshape grad_output to (B, N, 1)\n",
    "            grad_output = grad_output.unsqueeze(-1)  # (B, N, 1)\n",
    "        \n",
    "        # Reshape y to (B, N, 1)\n",
    "        y = y.unsqueeze(-1)  # (B, N, 1)\n",
    "        \n",
    "        # Compute the diagonal part of the Jacobian matrix: y * (1 - y)\n",
    "        diag_jacobian = y * (1 - y)  # (B, N, 1)\n",
    "        \n",
    "        # Compute the outer product part of the Jacobian matrix: -y * y^T\n",
    "        outer_product_jacobian = -y @ y.transpose(1, 2)  # (B, N, N)\n",
    "        \n",
    "        # Combine both parts to get the full Jacobian matrix\n",
    "        eye = torch.eye(num_classes).to(y.device)\n",
    "        jacobian_matrix = diag_jacobian * eye + outer_product_jacobian * (1- eye)  # (B, N, N)\n",
    "        \n",
    "        if grad_output is not None:\n",
    "            # Multiply the Jacobian matrix with grad_output\n",
    "            dx = torch.matmul(jacobian_matrix, grad_output).squeeze(-1)  # (B, N)\n",
    "            return dx\n",
    "        return jacobian_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropy:\n",
    "    def __init__(self, fast_backward=False):\n",
    "        self.softmax = SoftMax()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, y: torch.Tensor):\n",
    "        # x: (B, N), y: (B,)\n",
    "        batch_size, num_classes = x.shape\n",
    "        self.batch_size = batch_size\n",
    "        x = self.softmax.forward(x)  # (B, N) -> (B, N)\n",
    "        self.preds = x\n",
    "        print(f\"SoftMax outputs: {x}\")\n",
    "        one_hot_y = F.one_hot(y, num_classes=num_classes)\n",
    "        self.targets = one_hot_y\n",
    "        H = -torch.log(x)\n",
    "        loss = (H * one_hot_y).sum() / batch_size\n",
    "        return loss\n",
    "\n",
    "    def backward(self):\n",
    "        return (self.preds - self.targets) / self.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6590, 0.2424, 0.0986]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([2.0, 1.0, 0.1], requires_grad=True)\n",
    "softmax = SoftMax()\n",
    "y = softmax.forward(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2247, -0.1598, -0.0650],\n",
       "         [-0.1598,  0.1837, -0.0239],\n",
       "         [-0.0650, -0.0239,  0.0889]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax.backward(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([ 0.2247, -0.1598, -0.0650]),)\n",
      "(tensor([-0.1598,  0.1837, -0.0239]),)\n",
      "(tensor([-0.0650, -0.0239,  0.0889]),)\n"
     ]
    }
   ],
   "source": [
    "print(torch.autograd.grad(y[0, 0], x, retain_graph = True))\n",
    "print(torch.autograd.grad(y[0, 1], x, retain_graph = True))\n",
    "print(torch.autograd.grad(y[0, 2], x, retain_graph = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SoftMax outputs: tensor([[0.2894, 0.3199, 0.3907],\n",
      "        [0.3199, 0.2894, 0.3907],\n",
      "        [0.3548, 0.3548, 0.2905]], grad_fn=<DivBackward0>)\n",
      "My CE: 1.1719828844070435\n",
      "Torch CE: 1.1719828844070435\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[0.2,0.3,0.5],[0.3,0.2,0.5],[0.4,0.4,0.2]], requires_grad=True)\n",
    "y = torch.tensor([1,0,2])\n",
    "ce = CrossEntropy()\n",
    "loss = ce.forward(x, y)\n",
    "print(f\"My CE: {loss}\")\n",
    "_loss = F.cross_entropy(x, y)\n",
    "print(f\"Torch CE: {_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0965, -0.2267,  0.1302],\n",
       "        [-0.2267,  0.0965,  0.1302],\n",
       "        [ 0.1183,  0.1183, -0.2365]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ce.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[ 0.0965, -0.2267,  0.1302],\n",
      "        [-0.2267,  0.0965,  0.1302],\n",
      "        [ 0.1183,  0.1183, -0.2365]]),)\n"
     ]
    }
   ],
   "source": [
    "print(torch.autograd.grad(loss, x, retain_graph = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "85ab25dab3758848e7da8898424f64a7a1240aaea70eeb60f1d88417527cc5d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
